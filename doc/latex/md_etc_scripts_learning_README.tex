This is a phase prior to the normal E\+AR utilization, and it is called learning phase since is a kind of hardware characterization of the nodes.

During the learning phase a matrix of coefficients which allows to predict the power and performance of each repetitive iteration, is computed on all nodes and stored. To get them, during the phase, a set of preselected stress tools or benchmarks (also called kernels), included in the E\+AR package, are executed using the selected range of frequencies of the system.

{\bfseries N\+O\+TE\+:} A set of scripts to simplify the learning phase is installed the {\ttfamily bin} folder. However, the details of its functionality will be discussed later. Before of that it is better to understand what is the learning phase. This note was written because those scripts will be referenced during the learning phase overview.

The learning phase consists in four major steps.

\subsection*{Step 1, kernel compilation }

The first step is the kernel compilation. In case you want to compile one by one manually, open the script {\ttfamily bin/scripts/learning/helpers/kernels\+\_\+executor.\+sh} and look at the function {\ttfamily learning\+\_\+phase()}. Here you will find all the paths and compile instructions (make) and install (move) each kernel when {\ttfamily \$\+B\+E\+N\+C\+H\+\_\+\+M\+O\+DE} variable is set to {\ttfamily compile}.

As you can see there are some confusing letters and numbers. This characters are related with the kernel customization, looking for maximize the nodes stress to obtain the maximum quality coefficients.

This customization will take the number of processes to fill the total number of C\+P\+US (not counting hyperthreading). Also a class letter defining the stress level of the benchmark could be required. The class letter goes from lighter levels of stress (A) to heavier levels (D, E...).

You can see some information about all the kernels in the following table\+:

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*2{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf Reference  }\\\cline{1-2}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf Reference  }\\\cline{1-2}
\endhead
dgemm &\href{https://software.intel.com/en-us/mkl-tutorial-c-multiplying-matrices-using-dgemm}{\tt https\+://software.\+intel.\+com/en-\/us/mkl-\/tutorial-\/c-\/multiplying-\/matrices-\/using-\/dgemm} \\\cline{1-2}
stream &\href{http://www.cs.virginia.edu/stream/ref.html}{\tt http\+://www.\+cs.\+virginia.\+edu/stream/ref.\+html} \\\cline{1-2}
bt-\/mz &\href{https://www.nas.nasa.gov/publications/npb.html}{\tt https\+://www.\+nas.\+nasa.\+gov/publications/npb.\+html} \\\cline{1-2}
sp-\/mz &\char`\"{} 
$<$tr$>$$<$td$>$ lu-\/mz  $<$td$>$ \char`\"{} \\\cline{1-2}
ep &\char`\"{} 
$<$tr$>$$<$td$>$ lu     $<$td$>$ \char`\"{} \\\cline{1-2}
ua &" \\\cline{1-2}
\end{longtabu}
\subsection*{Step 2, kernel launch }

To launch the kernels the cluster queue manager S\+L\+U\+RM will be used. To complete the learning phase all the kernels will have to be launched in every node of your cluster in the different selected frequencies. You can also maximize the precision of the gathered data by repeating the execution more than 1 time.

It is required the {\ttfamily srun} S\+L\+U\+RM\textquotesingle{}s command or {\ttfamily mpirun} bootstraping with S\+L\+U\+RM. The {\ttfamily -\/-\/ear-\/learning=\$\+P\+\_\+\+S\+T\+A\+TE} parameter next to the selected P\+\_\+\+S\+T\+A\+TE, allows the E\+AR S\+L\+U\+RM\textquotesingle{}s plugin to execute a kernel in learning phase mode.

These are two examples of the {\ttfamily srun} and {\ttfamily mpirun} commands for a node of 40 C\+P\+Us\+:

{\ttfamily srun -\/N 1 -\/n 40 -\/J \char`\"{}bt-\/mz\char`\"{} -\/w node1001 -\/-\/ear-\/policy=M\+O\+N\+I\+T\+O\+R\+I\+N\+G\+\_\+\+O\+N\+LY -\/-\/ear-\/learning=1 /installation.path/bin/kernels/bt-\/mz.\+C.\+40}

`mpirun -\/n 40 -\/bootstrap slurm -\/bootstrap-\/exec-\/args="-\/J \textquotesingle{}bt-\/mz\textquotesingle{} -\/w node1001 --ear-\/verbose=1 --ear-\/learning=1 /installation.path/bin/kernels/bt-\/mz.\+C.\+40`

It is required to use the same name for a kernel launched at different P\+\_\+\+S\+T\+A\+T\+Es, because the binary which calculates the coefficients employs the name to classify the kernels.

\subsection*{Step 3, kernel customization }

If after the launching of a learning phase kernel at P\+\_\+\+S\+T\+A\+TE 1 the elapsed (in seconds) is between 60 and 120, then it is good quality kernel. In case some benchmarks are not between these times, you can increase or decrease the class letter at compilation time.

If no letter can adjust the kernels to your node, you can surf to every kernel configuration time and switch the values summarized in the following table\+:

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*4{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf File }&{\bf Function }&{\bf Var  }\\\cline{1-4}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf File }&{\bf Function }&{\bf Var  }\\\cline{1-4}
\endhead
bt-\/mz &N\+P\+B3.\+3.\+1-\/\+M\+Z/\+N\+P\+B3.\+3-\/\+M\+Z-\/\+M\+P\+I/sys/setparams.\+c &write\+\_\+bt\+\_\+info &niter \\\cline{1-4}
lu-\/mz &N\+P\+B3.\+3.\+1-\/\+M\+Z/\+N\+P\+B3.\+3-\/\+M\+Z-\/\+M\+P\+I/sys/setparams.\+c &write\+\_\+lu\+\_\+info &itmax \\\cline{1-4}
sp-\/mz &N\+P\+B3.\+3.\+1-\/\+M\+Z/\+N\+P\+B3.\+3-\/\+M\+Z-\/\+M\+P\+I/sys/setparams.\+c &write\+\_\+sp\+\_\+info &niter \\\cline{1-4}
ep &N\+P\+B3.\+3.\+1/\+N\+P\+B3.3-\/\+M\+P\+I/sys/setparams.\+c &write\+\_\+ep\+\_\+info &m \\\cline{1-4}
lu &N\+P\+B3.\+3.\+1/\+N\+P\+B3.3-\/\+M\+P\+I/sys/setparams.\+c &write\+\_\+lu\+\_\+info &itmax \\\cline{1-4}
ua &N\+P\+B3.\+3.\+1/\+N\+P\+B3.3-\/\+O\+M\+P/sys/setparams.\+c &write\+\_\+ua\+\_\+info &niter \\\cline{1-4}
\end{longtabu}
Depending on your system you have to increase or decrease its value. As a reference, it is provided a table containing the letter for the script and the value of the variable for a couple of C\+PU architectures\+:

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*3{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf Haswell }&{\bf Skylake  }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}{\bf Kernel }&{\bf Haswell }&{\bf Skylake  }\\\cline{1-3}
\endhead
bt-\/mz &C / 200 &C / 600 \\\cline{1-3}
lu-\/mz &C / 250 &C / 800 \\\cline{1-3}
sp-\/mz &C / 800 &C / 2000 \\\cline{1-3}
ep &D / 36 &D / 37 \\\cline{1-3}
lu &C / 250 &C / 750 \\\cline{1-3}
ua &C / 200 &C / 200 \\\cline{1-3}
\end{longtabu}
For {\ttfamily dgemm}, you have to edit the file ‘dgemm\+\_\+example.\+f’. Take a look to P\+A\+R\+A\+M\+E\+T\+ER variable definition in the first line, which sets the size of the computing matrix. Increase or decrease that values equally depending if you want to add or subtract computing time.

Once the customization is done, you have to run again your customized kernels to complete the learning phase. Also, it is recommended to clean the customized kernels records of your database.

\subsection*{Step 4, coefficients computing }

Once launched all the kernels at the different frequencies (or P\+\_\+\+S\+T\+A\+T\+ES), the coefficients have to be computed using the installed binary {\ttfamily /bin/compute\+\_\+coefficients}.

This binary will compute the coefficients and also store the file in the location specified by the configuration file {\ttfamily ear.\+conf}. There is just one file per node, so the binary have to be run one time per node in a node of the same hardware architecture, because it checks the range of P\+\_\+\+S\+T\+A\+T\+Es.

The path of the coefficients, the nominal frequency of the node an also de node name have to be passed to correctly compute the coefficients. In case the node name is not present, the binary will get it\textquotesingle{}s the host name.

This is an example\+: {\ttfamily ./compute\+\_\+coefficients /etc/ear/coeffs 2400000 node1001}

Remember to load the E\+AR module, which specifies the location of the {\ttfamily ear.\+conf} configuration file.

\subsection*{Automatized kernels compilation script }

A set of scripts are provided for speed up with minimum edition requirements. These files are placed in the {\ttfamily bin/scripts} folder in your E\+AR installation folder. For a list of all provided scripts visit https\+://github.com/\+Barcelona\+Supercomputing\+Center/\+E\+A\+R/blob/development/etc/scripts/\+R\+E\+A\+D\+M\+E.\+md \char`\"{}\+E\+A\+R scripts page\char`\"{}.

The compiling script is located in {\ttfamily bin/scripts/learning/learning\+\_\+phase\+\_\+compile.\+sh}. Before execute it, you have to perform some adjustments\+: 1) Open {\ttfamily learning\+\_\+phase\+\_\+compile.\+sh} and look for these lines\+: 
\begin{DoxyCode}
1 # Edit architecture values
2 export CORES=28
3 export SOCKETS=2
4 export CORES\_PER\_SOCKET=14
\end{DoxyCode}
 2) Update the following parameters\+:~\newline

\begin{DoxyItemize}
\item {\bfseries C\+O\+R\+ES}\+: the total number of cores in a single computing node.~\newline

\item {\bfseries S\+O\+C\+K\+E\+TS}\+: the total number of sockets in a single computing node.~\newline

\item {\bfseries C\+O\+R\+E\+S\+\_\+\+P\+E\+R\+\_\+\+S\+O\+C\+K\+ET}\+: the total number of cores per socket in a single computing node.~\newline
 3) Launch the compiling phase by typing {\ttfamily ./learning\+\_\+phase\+\_\+compile.sh} in your compile node.
\end{DoxyItemize}

Also you can easily customize your kernels by adjusting the script located in {\ttfamily bin/scripts/learning/helpers/kernels\+\_\+iterator.\+sh}. For example if you want to increase its execution time of a kernel compiled with class letter C, switch it by D. Or if you want to decrease the execution time of a kernel compiled with class letter B, switch the letter by A. Then compile and execute again.

You could see where you have to edit in the following example\+: 
\begin{DoxyCode}
1 learning-phase lu-mpi C
2 learning-phase ep D
3 learning-phase bt-mz C
4 learning-phase sp-mz C
5 learning-phase lu-mz C
6 learning-phase ua C
7 learning-phase dgemm
8 learning-phase stream
\end{DoxyCode}


As you can see, there are no class letters for {\ttfamily dgemm} or {\ttfamily stream} kernels. Stream is a well known benchmark and there is no need to manual modification because varies its behavior itself. For {\ttfamily dgemm} or for a class letter benchmark which doesn’t fit in your goals, it’s recommended to do a manual kernel modification.

\subsection*{Automatized kernels execution script }

Next to the kernels compilation script, the executing version is also provided. Having the kernels compiled, installed and tested, you are ready to execute the learning phase.

Before that, you have to perform some adjustments\+: 1) Open the script {\ttfamily bin/scripts/learning/learning\+\_\+phase\+\_\+execute.\+sh}. 2) Look at these lines 
\begin{DoxyCode}
1 # Edit architecture values
2 export CORES=28
3 export SOCKETS=2
4 export CORES\_PER\_SOCKET=14
5 
6 # Edit learning phase parameters
7 export EAR\_MIN\_P\_STATE=1
8 export EAR\_MAX\_P\_STATE=6
\end{DoxyCode}
 3) Update the following parameters\+:~\newline

\begin{DoxyItemize}
\item {\bfseries C\+O\+R\+ES}\+: the total number of cores in a single computing node.~\newline

\item {\bfseries S\+O\+C\+K\+E\+TS}\+: the total number of sockets in a single computing node.~\newline

\item {\bfseries C\+O\+R\+E\+S\+\_\+\+P\+E\+R\+\_\+\+S\+O\+C\+K\+ET}\+: the total number of cores per socket in a single computing node.~\newline

\item {\bfseries E\+A\+R\+\_\+\+M\+I\+N\+\_\+\+P\+\_\+\+S\+T\+A\+TE}\+: defines the maximum frequency to set during the learning phase. The default value is 1, meaning that the nominal frequency will be the maximum frequency that your cluster nodes will set. In the current version of E\+AR turbo support is not included.~\newline

\item {\bfseries E\+A\+R\+\_\+\+M\+A\+X\+\_\+\+P\+\_\+\+S\+T\+A\+TE}\+: defines the minimum frequency to test during the learning phase. If 6 is set and E\+A\+R\+\_\+\+M\+I\+N\+\_\+\+P\+\_\+\+S\+T\+A\+TE is 1, it means that 6 frequencies will be set during the learning phase, from 1 to 6. This set of frequencies have to match with the set of frequencies that your cluster nodes are able to set during computing time.~\newline
 4) Edit the execution command located in {\ttfamily bin/scripts/learning/helpers/kernels\+\_\+executor.\+sh} in the function {\ttfamily launching\+\_\+slurm()}. By default it will use the {\ttfamily srun} command, but you can switch it by other one, like {\ttfamily mpirun}. Just try to figure how to translate the written command to yours. 5) Execute the learning phase in all of your nodes by typing a command like\+: {\ttfamily ./learning\+\_\+phase\+\_\+execute.sh $<$hostlist$>$}, passing a the path of a file containing the list of nodes where you want to perform the learning phase. An {\ttfamily sbatch} will be launched exclusively in every node, performing a {\ttfamily srun} series of the kernel in the same node. 6) Execute the coefficients compute binary by typing {\ttfamily ./learning\+\_\+phase\+\_\+coeffs.sh $<$hostlist$>$} in a node which shares the same architecture (or at least the P\+\_\+\+S\+T\+A\+T\+Es list) of the nodes of the completed learning phase. 7) Check that there are the correct number of coefficients in the selected coefficients installation path. 
\end{DoxyItemize}